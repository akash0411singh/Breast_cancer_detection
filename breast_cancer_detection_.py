# -*- coding: utf-8 -*-
"""Breast Cancer Detection .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GRcoiV4eOjEV_Q27tQ2fwFcY32MeaZ9x

#**This program detects breast cancer, based off  data.**

# **Importing  libraries**
"""

#import libraries 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns

"""# **Importing dataset** """

#Load the data
from google.colab import files # Use to load data on Google Colab
uploaded = files.upload() # Use to load data on Google Colab 
df = pd.read_csv('data.csv') 
df.head(7)

"""# **Data exploration and handling missing values** """

#Count the number of rows and columns in the data set
df.shape

#Count the empty (NaN, NAN, na) values in each column
df.isna().sum()

#Drop the column with all missing values (na, NAN, NaN)
df=df.dropna(axis = 1)
df.shape

#Get a count of the number of 'Malignant(M)' & 'Bengin(B)' cells
df['diagnosis'].value_counts()

#visualise the count.
print(sns.countplot(df['diagnosis'], label ="Count"))

df.dtypes

df.head(5)

sns.pairplot(data = df,hue = "diagnosis",vars = ['radius_mean','texture_mean','area_mean','perimeter_mean','smoothness_mean','compactness_mean'])

df.info()

#Get the correlation of the columns
df.corr()

plt.figure(figsize=(20,20))  
sns.heatmap(df.corr(), annot=True, fmt='.0%')

"""# **Data processing** """

#Encoding categorical data values i.e. diagnosis. 
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df.iloc[:,1]= le.fit_transform(df.iloc[:,1].values)
print(df.iloc[:,1].values)

#Now splitting dataset into dependent and independent dataset.

X = df.iloc[:, 2:31].values 
y = df.iloc[:, 1].values

from sklearn.model_selection import train_test_split
X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 0)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Using DecisionTreeClassifier 
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
  tree.fit(X_train, y_train)
  y_pred=tree.predict(X_test)

"""# **Training different model on the Training set** """

def models(X_train , y_train):
  #using logistic regression.
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state = 0)
  log.fit(X_train , y_train)

  #using KNeighnors classification.
  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
  knn.fit(X_train, y_train)

  #Using SVC linear
  from sklearn.svm import SVC
  svc_lin = SVC(kernel = 'linear', random_state = 0)
  svc_lin.fit(X_train, y_train)

  #Using SVC rbf
  from sklearn.svm import SVC
  svc_rbf = SVC(kernel = 'rbf', random_state = 0)
  svc_rbf.fit(X_train, y_train)

  #Using GaussianNB 
  from sklearn.naive_bayes import GaussianNB
  gauss = GaussianNB()
  gauss.fit(X_train, y_train)

  
  #Using DecisionTreeClassifier 
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
  tree.fit(X_train, y_train)

  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm
  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
  forest.fit(X_train, y_train)


    #print model accuracy on the training data.
  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, y_train))
  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, y_train))
  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, y_train))
  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, y_train))
  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, y_train))
  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, y_train))
  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, y_train))

  return log , knn, svc_lin, svc_rbf, gauss, tree, forest

model = models(X_train,y_train)
print(model)

from sklearn.metrics import confusion_matrix
for i in range(len(model)):
  cm = confusion_matrix(y_test, model[i].predict(X_test))
  
  TN = cm[0][0]
  TP = cm[1][1]
  FN = cm[1][0]
  FP = cm[0][1]
  
  print(cm)
  print('Model[{}] Testing Accuracy = "{}!"'.format(i,  (TP + TN) / (TP + TN + FN + FP)))
  print()# Print a new line

#Show other ways to get the classification accuracy & other metrics 
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range(len(model)):
  print('Model ',i)
  #Check precision, recall, f1-score
  print( classification_report(y_test, model[i].predict(X_test)) )
  #Another way to get the models accuracy on the test data
  print( accuracy_score(y_test, model[i].predict(X_test)))
  print()#Print a new line

"""#Printing Prediction of test set  by Support Vector Machine (RBF Classifier) model """

#Print Prediction of Support Vector Machine (RBF Classifier) model
y_pred = model[3].predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

model[3]

"""#**IMPROVING THE MODEL - PART 2**

##Applying k-Fold Cross ValidationÂ¶
"""

from sklearn.model_selection import cross_val_score
accuracies= cross_val_score (estimator = model[3] , X=X_train ,y=y_train , cv =10)
print("Accuracy: {:.2f}%".format(accuracies.mean()*100)) #this print accuracy in percentge format. important code****.
print("Standard deviation: {:.2f}%".format(accuracies.std()*100))

from sklearn.model_selection import GridSearchCV
parameters=[{'C':[0.25,0.5,0.75,1,10,100],'kernel':["rbf"],'gamma':[0.1,0.2,0.,0.4,0.5,0.6,0.7,0.8,0.9]}]
grid_search = GridSearchCV(estimator= model[3],
                          param_grid = parameters , 
                          scoring = 'accuracy',
                          cv = 10,
                          n_jobs = -1)
grid_search.fit(X_train, y_train)
best_accuracy = grid_search.best_score_
best_parameter = grid_search.best_params_

print(best_parameter)

print(best_accuracy )

grid_predictions = grid_search.predict(X_test)

cm = confusion_matrix(y_test, grid_predictions)

sns.heatmap(cm, annot=True)

print(classification_report(y_test,grid_predictions))

"""# ***That's pretty good accuracy.***"""